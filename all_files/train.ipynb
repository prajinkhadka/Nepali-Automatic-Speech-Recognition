{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13123ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import load_dataset, load_metric,Dataset,concatenate_datasets,set_caching_enabled, ClassLabel\n",
    "import pandas as pd\n",
    "\n",
    "import random\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import json\n",
    "from transformers import Wav2Vec2CTCTokenizer,Wav2Vec2ForCTC,Wav2Vec2Processor,Trainer,TrainingArguments,Wav2Vec2FeatureExtractor\n",
    "\n",
    "import re\n",
    "set_caching_enabled(False)\n",
    "\n",
    "import soundfile as sf\n",
    "import torchaudio\n",
    "\n",
    "\n",
    "import IPython.display as ipd\n",
    "import numpy as np\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "# Set environment variables\n",
    "os.environ['WANDB_DISABLED '] = 'True'\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79956b38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import logging\n",
    "import transformers\n",
    "transformers.logging.get_verbosity = lambda: logging.NOTSET\n",
    "\n",
    "transformers.logging.get_verbosity()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9d5c043",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "datasets.logging.get_verbosity = lambda: logging.NOTSET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a2c68ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Wav2Vec2CTCTokenizer(\"./vocab.json\", unk_token=\"[UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acdd9cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85565677",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "957cacd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob \n",
    "data_dir  = \"/workspace/data_processed/*\"\n",
    "data_dir_list = glob.glob(data_dir)\n",
    "data_dir_list.remove('/workspace/data_processed/5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f33e8dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/workspace/data_processed/26',\n",
       " '/workspace/data_processed/13',\n",
       " '/workspace/data_processed/2',\n",
       " '/workspace/data_processed/7',\n",
       " '/workspace/data_processed/14',\n",
       " '/workspace/data_processed/24',\n",
       " '/workspace/data_processed/3',\n",
       " '/workspace/data_processed/38',\n",
       " '/workspace/data_processed/28',\n",
       " '/workspace/data_processed/16',\n",
       " '/workspace/data_processed/30',\n",
       " '/workspace/data_processed/36',\n",
       " '/workspace/data_processed/4',\n",
       " '/workspace/data_processed/18',\n",
       " '/workspace/data_processed/29',\n",
       " '/workspace/data_processed/8',\n",
       " '/workspace/data_processed/31',\n",
       " '/workspace/data_processed/27',\n",
       " '/workspace/data_processed/6',\n",
       " '/workspace/data_processed/10',\n",
       " '/workspace/data_processed/21',\n",
       " '/workspace/data_processed/15',\n",
       " '/workspace/data_processed/35',\n",
       " '/workspace/data_processed/9',\n",
       " '/workspace/data_processed/25',\n",
       " '/workspace/data_processed/22',\n",
       " '/workspace/data_processed/33',\n",
       " '/workspace/data_processed/20',\n",
       " '/workspace/data_processed/12',\n",
       " '/workspace/data_processed/39',\n",
       " '/workspace/data_processed/1',\n",
       " '/workspace/data_processed/37',\n",
       " '/workspace/data_processed/17',\n",
       " '/workspace/data_processed/11',\n",
       " '/workspace/data_processed/19',\n",
       " '/workspace/data_processed/34',\n",
       " '/workspace/data_processed/23',\n",
       " '/workspace/data_processed/32']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "efcb0204",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set_list_total = []\n",
    "for i in range(len(data_dir_list)):\n",
    "    common_voice_val = Dataset.load_from_disk(data_dir_list[i])\n",
    "    data_set_list_total.append(common_voice_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3bae94e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_set_list_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5a5ed13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len((data_set_list_total[:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2494ea84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len((data_set_list_total[-1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8aedfca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_voice_train = concatenate_datasets(data_set_list_total[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "948ab852",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_voice_test = concatenate_datasets(data_set_list_total[-1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df877bcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_values', 'labels'],\n",
       "    num_rows: 323525\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_voice_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce171520",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_values', 'labels'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_voice_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2860e6db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_values', 'labels'],\n",
       "    num_rows: 323525\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_voice_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa254463",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b043c5254e624907a32f69b597ae656d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center>\\n<img src=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d4d7501e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_values', 'labels'],\n",
       "    num_rows: 323525\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_voice_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5a1a8afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_name = \"wav2vec2-large-xlsr-300m-nepali\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5e1b40df",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.push_to_hub(repo_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6996db40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "    Args:\n",
    "        processor (:class:`~transformers.Wav2Vec2Processor`)\n",
    "            The processor used for proccessing the data.\n",
    "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence if provided).\n",
    "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
    "              maximum acceptable input length for the model if that argument is not provided.\n",
    "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
    "              different lengths).\n",
    "        max_length (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).\n",
    "        max_length_labels (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``labels`` returned list and optionally padding length (see above).\n",
    "        pad_to_multiple_of (:obj:`int`, `optional`):\n",
    "            If set will pad the sequence to a multiple of the provided value.\n",
    "            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n",
    "            7.5 (Volta).\n",
    "    \"\"\"\n",
    "\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "    max_length: Optional[int] = None\n",
    "    max_length_labels: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    pad_to_multiple_of_labels: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lenghts and need\n",
    "        # different padding methods\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                max_length=self.max_length_labels,\n",
    "                pad_to_multiple_of=self.pad_to_multiple_of_labels,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a496ea8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7d06a4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "wer_metric = load_metric(\"wer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e33bcb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a6286e01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(processor.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1142595f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/wav2vec2-xls-r-300m were not used when initializing Wav2Vec2ForCTC: ['quantizer.weight_proj.bias', 'project_q.bias', 'quantizer.codevectors', 'quantizer.weight_proj.weight', 'project_q.weight', 'project_hid.bias', 'project_hid.weight']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-xls-r-300m and are newly initialized: ['lm_head.bias', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2ForCTC\n",
    "\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    \"facebook/wav2vec2-xls-r-300m\", \n",
    "    attention_dropout=0.0,\n",
    "    hidden_dropout=0.0,\n",
    "    feat_proj_dropout=0.0,\n",
    "    mask_time_prob=0.05,\n",
    "    layerdrop=0.0,\n",
    "    ctc_loss_reduction=\"mean\", \n",
    "    pad_token_id=processor.tokenizer.pad_token_id,\n",
    "    vocab_size=len(processor.tokenizer),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "15b3a493",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.freeze_feature_extractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "40356f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  output_dir=repo_name,\n",
    "  group_by_length=True,\n",
    "  per_device_train_batch_size=16,\n",
    "  gradient_accumulation_steps=2,\n",
    "  evaluation_strategy=\"steps\",\n",
    "  num_train_epochs=100,\n",
    "  gradient_checkpointing=True,\n",
    "  fp16=True,\n",
    "  save_steps=400,\n",
    "  eval_steps=400,\n",
    "  logging_steps=400,\n",
    "  learning_rate=3e-4,\n",
    "  warmup_steps=500,\n",
    "  save_total_limit=2,\n",
    "  push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "def2ef03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/wav2vec2-large-xlsr-300m-nepali is already a clone of https://huggingface.co/prajin/wav2vec2-large-xlsr-300m-nepali. Make sure you pull the latest changes with `repo.git_pull()`.\n",
      "Using amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=common_voice_train,\n",
    "    eval_dataset=common_voice_test,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4723200e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 323525\n",
      "  Num Epochs = 100\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 1011000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='35201' max='1011000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  35201/1011000 34:50:45 < 966:00:39, 0.28 it/s, Epoch 3.48/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>6.657500</td>\n",
       "      <td>3.322663</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>2.041400</td>\n",
       "      <td>1.162285</td>\n",
       "      <td>0.871682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.109100</td>\n",
       "      <td>0.900162</td>\n",
       "      <td>0.734135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.975700</td>\n",
       "      <td>0.791136</td>\n",
       "      <td>0.683623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.891200</td>\n",
       "      <td>0.740895</td>\n",
       "      <td>0.642548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.841800</td>\n",
       "      <td>0.697942</td>\n",
       "      <td>0.606249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.831700</td>\n",
       "      <td>0.676051</td>\n",
       "      <td>0.591271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.796000</td>\n",
       "      <td>0.658611</td>\n",
       "      <td>0.568378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.789000</td>\n",
       "      <td>0.649247</td>\n",
       "      <td>0.562919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.768300</td>\n",
       "      <td>0.634902</td>\n",
       "      <td>0.564559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.759300</td>\n",
       "      <td>0.616495</td>\n",
       "      <td>0.547183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.759000</td>\n",
       "      <td>0.618201</td>\n",
       "      <td>0.554057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.749000</td>\n",
       "      <td>0.601834</td>\n",
       "      <td>0.536016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.750800</td>\n",
       "      <td>0.593987</td>\n",
       "      <td>0.526071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.735100</td>\n",
       "      <td>0.586008</td>\n",
       "      <td>0.535683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.720500</td>\n",
       "      <td>0.598600</td>\n",
       "      <td>0.514255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>0.734500</td>\n",
       "      <td>0.571130</td>\n",
       "      <td>0.512482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>0.717400</td>\n",
       "      <td>0.567282</td>\n",
       "      <td>0.528443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>0.716300</td>\n",
       "      <td>0.592329</td>\n",
       "      <td>0.519855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.709500</td>\n",
       "      <td>0.578058</td>\n",
       "      <td>0.533336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>0.713900</td>\n",
       "      <td>0.571583</td>\n",
       "      <td>0.509603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>0.713000</td>\n",
       "      <td>0.562234</td>\n",
       "      <td>0.516918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9200</td>\n",
       "      <td>0.707100</td>\n",
       "      <td>0.566018</td>\n",
       "      <td>0.519314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9600</td>\n",
       "      <td>0.704800</td>\n",
       "      <td>0.581556</td>\n",
       "      <td>0.514929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.699300</td>\n",
       "      <td>0.549075</td>\n",
       "      <td>0.500607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10400</td>\n",
       "      <td>0.657200</td>\n",
       "      <td>0.536581</td>\n",
       "      <td>0.498552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10800</td>\n",
       "      <td>0.665900</td>\n",
       "      <td>0.551517</td>\n",
       "      <td>0.498494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11200</td>\n",
       "      <td>0.646900</td>\n",
       "      <td>0.538649</td>\n",
       "      <td>0.489041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11600</td>\n",
       "      <td>0.661700</td>\n",
       "      <td>0.552628</td>\n",
       "      <td>0.486636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.658100</td>\n",
       "      <td>0.530079</td>\n",
       "      <td>0.481301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12400</td>\n",
       "      <td>0.651000</td>\n",
       "      <td>0.523544</td>\n",
       "      <td>0.478173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12800</td>\n",
       "      <td>0.659400</td>\n",
       "      <td>0.511975</td>\n",
       "      <td>0.484464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13200</td>\n",
       "      <td>0.652200</td>\n",
       "      <td>0.527160</td>\n",
       "      <td>0.479654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13600</td>\n",
       "      <td>0.663800</td>\n",
       "      <td>0.521775</td>\n",
       "      <td>0.487135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.668900</td>\n",
       "      <td>0.532435</td>\n",
       "      <td>0.489806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14400</td>\n",
       "      <td>0.650900</td>\n",
       "      <td>0.514338</td>\n",
       "      <td>0.480686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14800</td>\n",
       "      <td>0.652800</td>\n",
       "      <td>0.519738</td>\n",
       "      <td>0.489998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15200</td>\n",
       "      <td>0.657500</td>\n",
       "      <td>0.513668</td>\n",
       "      <td>0.483540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15600</td>\n",
       "      <td>0.653200</td>\n",
       "      <td>0.510995</td>\n",
       "      <td>0.492902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.653800</td>\n",
       "      <td>0.518923</td>\n",
       "      <td>0.474569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16400</td>\n",
       "      <td>0.658400</td>\n",
       "      <td>0.504985</td>\n",
       "      <td>0.473238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16800</td>\n",
       "      <td>0.650900</td>\n",
       "      <td>0.506444</td>\n",
       "      <td>0.485870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17200</td>\n",
       "      <td>0.641000</td>\n",
       "      <td>0.512465</td>\n",
       "      <td>0.477956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17600</td>\n",
       "      <td>0.655400</td>\n",
       "      <td>0.491507</td>\n",
       "      <td>0.469002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.648100</td>\n",
       "      <td>0.520168</td>\n",
       "      <td>0.469834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18400</td>\n",
       "      <td>0.629200</td>\n",
       "      <td>0.486904</td>\n",
       "      <td>0.463593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18800</td>\n",
       "      <td>0.654700</td>\n",
       "      <td>0.512927</td>\n",
       "      <td>0.475926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19200</td>\n",
       "      <td>0.626800</td>\n",
       "      <td>0.493225</td>\n",
       "      <td>0.475460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19600</td>\n",
       "      <td>0.633800</td>\n",
       "      <td>0.483911</td>\n",
       "      <td>0.469302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.641300</td>\n",
       "      <td>0.496735</td>\n",
       "      <td>0.473721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20400</td>\n",
       "      <td>0.629900</td>\n",
       "      <td>0.484275</td>\n",
       "      <td>0.463835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20800</td>\n",
       "      <td>0.595600</td>\n",
       "      <td>0.470276</td>\n",
       "      <td>0.462378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21200</td>\n",
       "      <td>0.604600</td>\n",
       "      <td>0.478547</td>\n",
       "      <td>0.455172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21600</td>\n",
       "      <td>0.599900</td>\n",
       "      <td>0.477560</td>\n",
       "      <td>0.461272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.608300</td>\n",
       "      <td>0.492500</td>\n",
       "      <td>0.462295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22400</td>\n",
       "      <td>0.594600</td>\n",
       "      <td>0.499315</td>\n",
       "      <td>0.454381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22800</td>\n",
       "      <td>0.607400</td>\n",
       "      <td>0.473194</td>\n",
       "      <td>0.456686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23200</td>\n",
       "      <td>0.599000</td>\n",
       "      <td>0.477644</td>\n",
       "      <td>0.456603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23600</td>\n",
       "      <td>0.601300</td>\n",
       "      <td>0.470010</td>\n",
       "      <td>0.454173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.605800</td>\n",
       "      <td>0.490342</td>\n",
       "      <td>0.446085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24400</td>\n",
       "      <td>0.611900</td>\n",
       "      <td>0.468896</td>\n",
       "      <td>0.456994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24800</td>\n",
       "      <td>0.610400</td>\n",
       "      <td>0.476762</td>\n",
       "      <td>0.449363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25200</td>\n",
       "      <td>0.598700</td>\n",
       "      <td>0.463977</td>\n",
       "      <td>0.452908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25600</td>\n",
       "      <td>0.602500</td>\n",
       "      <td>0.458435</td>\n",
       "      <td>0.448806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.610900</td>\n",
       "      <td>0.470245</td>\n",
       "      <td>0.454381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26400</td>\n",
       "      <td>0.610900</td>\n",
       "      <td>0.462353</td>\n",
       "      <td>0.446817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26800</td>\n",
       "      <td>0.611700</td>\n",
       "      <td>0.454297</td>\n",
       "      <td>0.446026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27200</td>\n",
       "      <td>0.606300</td>\n",
       "      <td>0.480419</td>\n",
       "      <td>0.445602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27600</td>\n",
       "      <td>0.604000</td>\n",
       "      <td>0.453925</td>\n",
       "      <td>0.443647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.600300</td>\n",
       "      <td>0.449969</td>\n",
       "      <td>0.442873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28400</td>\n",
       "      <td>0.597300</td>\n",
       "      <td>0.458659</td>\n",
       "      <td>0.436082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28800</td>\n",
       "      <td>0.612300</td>\n",
       "      <td>0.457076</td>\n",
       "      <td>0.443297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29200</td>\n",
       "      <td>0.597500</td>\n",
       "      <td>0.464052</td>\n",
       "      <td>0.446210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29600</td>\n",
       "      <td>0.601800</td>\n",
       "      <td>0.456322</td>\n",
       "      <td>0.443305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.603700</td>\n",
       "      <td>0.446470</td>\n",
       "      <td>0.438762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30400</td>\n",
       "      <td>0.598500</td>\n",
       "      <td>0.440370</td>\n",
       "      <td>0.437472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30800</td>\n",
       "      <td>0.559500</td>\n",
       "      <td>0.435245</td>\n",
       "      <td>0.437538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31200</td>\n",
       "      <td>0.559700</td>\n",
       "      <td>0.450966</td>\n",
       "      <td>0.441108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31600</td>\n",
       "      <td>0.574400</td>\n",
       "      <td>0.460796</td>\n",
       "      <td>0.433844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.567800</td>\n",
       "      <td>0.446999</td>\n",
       "      <td>0.435084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32400</td>\n",
       "      <td>0.556300</td>\n",
       "      <td>0.453138</td>\n",
       "      <td>0.435292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32800</td>\n",
       "      <td>0.557900</td>\n",
       "      <td>0.433473</td>\n",
       "      <td>0.431422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33200</td>\n",
       "      <td>0.564500</td>\n",
       "      <td>0.448122</td>\n",
       "      <td>0.431747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33600</td>\n",
       "      <td>0.564800</td>\n",
       "      <td>0.440217</td>\n",
       "      <td>0.436590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.567700</td>\n",
       "      <td>0.440426</td>\n",
       "      <td>0.432571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34400</td>\n",
       "      <td>0.568600</td>\n",
       "      <td>0.439044</td>\n",
       "      <td>0.428460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34800</td>\n",
       "      <td>0.571300</td>\n",
       "      <td>0.448026</td>\n",
       "      <td>0.437863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35200</td>\n",
       "      <td>0.562400</td>\n",
       "      <td>0.436585</td>\n",
       "      <td>0.436265</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to wav2vec2-large-xlsr-300m-nepali/checkpoint-400\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-400/config.json\n",
      "Model weights saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-400/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-400/preprocessor_config.json\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to wav2vec2-large-xlsr-300m-nepali/checkpoint-800\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-800/config.json\n",
      "Model weights saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-800/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-800/preprocessor_config.json\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to wav2vec2-large-xlsr-300m-nepali/checkpoint-1200\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-1200/config.json\n",
      "Model weights saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-1200/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-1200/preprocessor_config.json\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/preprocessor_config.json\n",
      "Deleting older checkpoint [wav2vec2-large-xlsr-300m-nepali/checkpoint-400] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to wav2vec2-large-xlsr-300m-nepali/checkpoint-1600\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-1600/config.json\n",
      "Model weights saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-1600/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-1600/preprocessor_config.json\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/preprocessor_config.json\n",
      "Deleting older checkpoint [wav2vec2-large-xlsr-300m-nepali/checkpoint-800] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to wav2vec2-large-xlsr-300m-nepali/checkpoint-2000\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-2000/config.json\n",
      "Model weights saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-2000/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-2000/preprocessor_config.json\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/preprocessor_config.json\n",
      "Deleting older checkpoint [wav2vec2-large-xlsr-300m-nepali/checkpoint-1200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to wav2vec2-large-xlsr-300m-nepali/checkpoint-2400\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-2400/config.json\n",
      "Model weights saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-2400/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-2400/preprocessor_config.json\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/preprocessor_config.json\n",
      "Deleting older checkpoint [wav2vec2-large-xlsr-300m-nepali/checkpoint-1600] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to wav2vec2-large-xlsr-300m-nepali/checkpoint-2800\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-2800/config.json\n",
      "Model weights saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-2800/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-2800/preprocessor_config.json\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/preprocessor_config.json\n",
      "Deleting older checkpoint [wav2vec2-large-xlsr-300m-nepali/checkpoint-2000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to wav2vec2-large-xlsr-300m-nepali/checkpoint-3200\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-3200/config.json\n",
      "Model weights saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-3200/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-3200/preprocessor_config.json\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/preprocessor_config.json\n",
      "Deleting older checkpoint [wav2vec2-large-xlsr-300m-nepali/checkpoint-2400] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to wav2vec2-large-xlsr-300m-nepali/checkpoint-3600\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-3600/config.json\n",
      "Model weights saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-3600/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-3600/preprocessor_config.json\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/preprocessor_config.json\n",
      "Deleting older checkpoint [wav2vec2-large-xlsr-300m-nepali/checkpoint-2800] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to wav2vec2-large-xlsr-300m-nepali/checkpoint-4000\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-4000/config.json\n",
      "Model weights saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-4000/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-4000/preprocessor_config.json\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/preprocessor_config.json\n",
      "Deleting older checkpoint [wav2vec2-large-xlsr-300m-nepali/checkpoint-3200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to wav2vec2-large-xlsr-300m-nepali/checkpoint-4400\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-4400/config.json\n",
      "Model weights saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-4400/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-4400/preprocessor_config.json\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/preprocessor_config.json\n",
      "Deleting older checkpoint [wav2vec2-large-xlsr-300m-nepali/checkpoint-3600] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to wav2vec2-large-xlsr-300m-nepali/checkpoint-4800\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-4800/config.json\n",
      "Model weights saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-4800/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-4800/preprocessor_config.json\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/preprocessor_config.json\n",
      "Deleting older checkpoint [wav2vec2-large-xlsr-300m-nepali/checkpoint-4000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to wav2vec2-large-xlsr-300m-nepali/checkpoint-5200\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-5200/config.json\n",
      "Model weights saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-5200/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-5200/preprocessor_config.json\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/preprocessor_config.json\n",
      "Deleting older checkpoint [wav2vec2-large-xlsr-300m-nepali/checkpoint-4400] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to wav2vec2-large-xlsr-300m-nepali/checkpoint-5600\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-5600/config.json\n",
      "Model weights saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-5600/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-5600/preprocessor_config.json\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/preprocessor_config.json\n",
      "Deleting older checkpoint [wav2vec2-large-xlsr-300m-nepali/checkpoint-4800] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to wav2vec2-large-xlsr-300m-nepali/checkpoint-6000\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-6000/config.json\n",
      "Model weights saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-6000/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-6000/preprocessor_config.json\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/preprocessor_config.json\n",
      "Deleting older checkpoint [wav2vec2-large-xlsr-300m-nepali/checkpoint-5200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to wav2vec2-large-xlsr-300m-nepali/checkpoint-6400\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-6400/config.json\n",
      "Model weights saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-6400/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-6400/preprocessor_config.json\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/preprocessor_config.json\n",
      "Deleting older checkpoint [wav2vec2-large-xlsr-300m-nepali/checkpoint-5600] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to wav2vec2-large-xlsr-300m-nepali/checkpoint-6800\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-6800/config.json\n",
      "Model weights saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-6800/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-6800/preprocessor_config.json\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/preprocessor_config.json\n",
      "Deleting older checkpoint [wav2vec2-large-xlsr-300m-nepali/checkpoint-6000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to wav2vec2-large-xlsr-300m-nepali/checkpoint-7200\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-7200/config.json\n",
      "Model weights saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-7200/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-7200/preprocessor_config.json\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/preprocessor_config.json\n",
      "Deleting older checkpoint [wav2vec2-large-xlsr-300m-nepali/checkpoint-6400] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to wav2vec2-large-xlsr-300m-nepali/checkpoint-7600\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-7600/config.json\n",
      "Model weights saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-7600/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-7600/preprocessor_config.json\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/preprocessor_config.json\n",
      "Deleting older checkpoint [wav2vec2-large-xlsr-300m-nepali/checkpoint-6800] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to wav2vec2-large-xlsr-300m-nepali/checkpoint-8000\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-8000/config.json\n",
      "Model weights saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-8000/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-8000/preprocessor_config.json\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/preprocessor_config.json\n",
      "Deleting older checkpoint [wav2vec2-large-xlsr-300m-nepali/checkpoint-7200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to wav2vec2-large-xlsr-300m-nepali/checkpoint-8400\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-8400/config.json\n",
      "Model weights saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-8400/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-8400/preprocessor_config.json\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/preprocessor_config.json\n",
      "Deleting older checkpoint [wav2vec2-large-xlsr-300m-nepali/checkpoint-7600] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to wav2vec2-large-xlsr-300m-nepali/checkpoint-8800\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-8800/config.json\n",
      "Model weights saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-8800/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-8800/preprocessor_config.json\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/preprocessor_config.json\n",
      "Deleting older checkpoint [wav2vec2-large-xlsr-300m-nepali/checkpoint-8000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to wav2vec2-large-xlsr-300m-nepali/checkpoint-9200\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-9200/config.json\n",
      "Model weights saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-9200/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-9200/preprocessor_config.json\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/preprocessor_config.json\n",
      "Deleting older checkpoint [wav2vec2-large-xlsr-300m-nepali/checkpoint-8400] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to wav2vec2-large-xlsr-300m-nepali/checkpoint-9600\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-9600/config.json\n",
      "Model weights saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-9600/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-9600/preprocessor_config.json\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/preprocessor_config.json\n",
      "Deleting older checkpoint [wav2vec2-large-xlsr-300m-nepali/checkpoint-8800] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to wav2vec2-large-xlsr-300m-nepali/checkpoint-10000\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-10000/config.json\n",
      "Model weights saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-10000/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-10000/preprocessor_config.json\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/preprocessor_config.json\n",
      "Deleting older checkpoint [wav2vec2-large-xlsr-300m-nepali/checkpoint-9200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to wav2vec2-large-xlsr-300m-nepali/checkpoint-10400\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-10400/config.json\n",
      "Model weights saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-10400/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-10400/preprocessor_config.json\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/preprocessor_config.json\n",
      "Deleting older checkpoint [wav2vec2-large-xlsr-300m-nepali/checkpoint-9600] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to wav2vec2-large-xlsr-300m-nepali/checkpoint-10800\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-10800/config.json\n",
      "Model weights saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-10800/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-10800/preprocessor_config.json\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/preprocessor_config.json\n",
      "Deleting older checkpoint [wav2vec2-large-xlsr-300m-nepali/checkpoint-10000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to wav2vec2-large-xlsr-300m-nepali/checkpoint-11200\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-11200/config.json\n",
      "Model weights saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-11200/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-11200/preprocessor_config.json\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/preprocessor_config.json\n",
      "Deleting older checkpoint [wav2vec2-large-xlsr-300m-nepali/checkpoint-10400] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to wav2vec2-large-xlsr-300m-nepali/checkpoint-11600\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-11600/config.json\n",
      "Model weights saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-11600/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-11600/preprocessor_config.json\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/preprocessor_config.json\n",
      "Deleting older checkpoint [wav2vec2-large-xlsr-300m-nepali/checkpoint-10800] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to wav2vec2-large-xlsr-300m-nepali/checkpoint-12000\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-12000/config.json\n",
      "Model weights saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-12000/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-12000/preprocessor_config.json\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/preprocessor_config.json\n",
      "Deleting older checkpoint [wav2vec2-large-xlsr-300m-nepali/checkpoint-11200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to wav2vec2-large-xlsr-300m-nepali/checkpoint-12400\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-12400/config.json\n",
      "Model weights saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-12400/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-12400/preprocessor_config.json\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/preprocessor_config.json\n",
      "Deleting older checkpoint [wav2vec2-large-xlsr-300m-nepali/checkpoint-11600] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to wav2vec2-large-xlsr-300m-nepali/checkpoint-12800\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-12800/config.json\n",
      "Model weights saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-12800/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-12800/preprocessor_config.json\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/preprocessor_config.json\n",
      "Deleting older checkpoint [wav2vec2-large-xlsr-300m-nepali/checkpoint-12000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to wav2vec2-large-xlsr-300m-nepali/checkpoint-13200\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-13200/config.json\n",
      "Model weights saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-13200/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-13200/preprocessor_config.json\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/preprocessor_config.json\n",
      "Deleting older checkpoint [wav2vec2-large-xlsr-300m-nepali/checkpoint-12400] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to wav2vec2-large-xlsr-300m-nepali/checkpoint-13600\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-13600/config.json\n",
      "Model weights saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-13600/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-13600/preprocessor_config.json\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/preprocessor_config.json\n",
      "Deleting older checkpoint [wav2vec2-large-xlsr-300m-nepali/checkpoint-12800] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to wav2vec2-large-xlsr-300m-nepali/checkpoint-14000\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-14000/config.json\n",
      "Model weights saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-14000/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-14000/preprocessor_config.json\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/preprocessor_config.json\n",
      "Deleting older checkpoint [wav2vec2-large-xlsr-300m-nepali/checkpoint-13200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to wav2vec2-large-xlsr-300m-nepali/checkpoint-14400\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-14400/config.json\n",
      "Model weights saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-14400/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-14400/preprocessor_config.json\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/preprocessor_config.json\n",
      "Deleting older checkpoint [wav2vec2-large-xlsr-300m-nepali/checkpoint-13600] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to wav2vec2-large-xlsr-300m-nepali/checkpoint-14800\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-14800/config.json\n",
      "Model weights saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-14800/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-14800/preprocessor_config.json\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/preprocessor_config.json\n",
      "Deleting older checkpoint [wav2vec2-large-xlsr-300m-nepali/checkpoint-14000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to wav2vec2-large-xlsr-300m-nepali/checkpoint-15200\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-15200/config.json\n",
      "Model weights saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-15200/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-15200/preprocessor_config.json\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/preprocessor_config.json\n",
      "Deleting older checkpoint [wav2vec2-large-xlsr-300m-nepali/checkpoint-14400] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to wav2vec2-large-xlsr-300m-nepali/checkpoint-15600\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-15600/config.json\n",
      "Model weights saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-15600/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-15600/preprocessor_config.json\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/preprocessor_config.json\n",
      "Deleting older checkpoint [wav2vec2-large-xlsr-300m-nepali/checkpoint-14800] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to wav2vec2-large-xlsr-300m-nepali/checkpoint-16000\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-16000/config.json\n",
      "Model weights saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-16000/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-16000/preprocessor_config.json\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/preprocessor_config.json\n",
      "Deleting older checkpoint [wav2vec2-large-xlsr-300m-nepali/checkpoint-15200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to wav2vec2-large-xlsr-300m-nepali/checkpoint-16400\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-16400/config.json\n",
      "Model weights saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-16400/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-16400/preprocessor_config.json\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/preprocessor_config.json\n",
      "Deleting older checkpoint [wav2vec2-large-xlsr-300m-nepali/checkpoint-15600] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to wav2vec2-large-xlsr-300m-nepali/checkpoint-16800\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-16800/config.json\n",
      "Model weights saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-16800/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-16800/preprocessor_config.json\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/preprocessor_config.json\n",
      "Deleting older checkpoint [wav2vec2-large-xlsr-300m-nepali/checkpoint-16000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to wav2vec2-large-xlsr-300m-nepali/checkpoint-17200\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-17200/config.json\n",
      "Model weights saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-17200/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-17200/preprocessor_config.json\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/preprocessor_config.json\n",
      "Deleting older checkpoint [wav2vec2-large-xlsr-300m-nepali/checkpoint-16400] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to wav2vec2-large-xlsr-300m-nepali/checkpoint-17600\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-17600/config.json\n",
      "Model weights saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-17600/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-17600/preprocessor_config.json\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/preprocessor_config.json\n",
      "Deleting older checkpoint [wav2vec2-large-xlsr-300m-nepali/checkpoint-16800] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to wav2vec2-large-xlsr-300m-nepali/checkpoint-18000\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-18000/config.json\n",
      "Model weights saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-18000/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-18000/preprocessor_config.json\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/preprocessor_config.json\n",
      "Deleting older checkpoint [wav2vec2-large-xlsr-300m-nepali/checkpoint-17200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to wav2vec2-large-xlsr-300m-nepali/checkpoint-18400\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-18400/config.json\n",
      "Model weights saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-18400/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-18400/preprocessor_config.json\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/preprocessor_config.json\n",
      "Deleting older checkpoint [wav2vec2-large-xlsr-300m-nepali/checkpoint-17600] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to wav2vec2-large-xlsr-300m-nepali/checkpoint-18800\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-18800/config.json\n",
      "Model weights saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-18800/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-18800/preprocessor_config.json\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/preprocessor_config.json\n",
      "Deleting older checkpoint [wav2vec2-large-xlsr-300m-nepali/checkpoint-18000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to wav2vec2-large-xlsr-300m-nepali/checkpoint-19200\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-19200/config.json\n",
      "Model weights saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-19200/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-19200/preprocessor_config.json\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/preprocessor_config.json\n",
      "Deleting older checkpoint [wav2vec2-large-xlsr-300m-nepali/checkpoint-18400] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to wav2vec2-large-xlsr-300m-nepali/checkpoint-19600\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-19600/config.json\n",
      "Model weights saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-19600/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-19600/preprocessor_config.json\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/preprocessor_config.json\n",
      "Deleting older checkpoint [wav2vec2-large-xlsr-300m-nepali/checkpoint-18800] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to wav2vec2-large-xlsr-300m-nepali/checkpoint-20000\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-20000/config.json\n",
      "Model weights saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-20000/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-20000/preprocessor_config.json\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/preprocessor_config.json\n",
      "Deleting older checkpoint [wav2vec2-large-xlsr-300m-nepali/checkpoint-19200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to wav2vec2-large-xlsr-300m-nepali/checkpoint-20400\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-20400/config.json\n",
      "Model weights saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-20400/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-20400/preprocessor_config.json\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/preprocessor_config.json\n",
      "Deleting older checkpoint [wav2vec2-large-xlsr-300m-nepali/checkpoint-19600] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to wav2vec2-large-xlsr-300m-nepali/checkpoint-20800\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-20800/config.json\n",
      "Model weights saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-20800/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-20800/preprocessor_config.json\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/preprocessor_config.json\n",
      "Deleting older checkpoint [wav2vec2-large-xlsr-300m-nepali/checkpoint-20000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to wav2vec2-large-xlsr-300m-nepali/checkpoint-21200\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-21200/config.json\n",
      "Model weights saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-21200/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-21200/preprocessor_config.json\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/preprocessor_config.json\n",
      "Deleting older checkpoint [wav2vec2-large-xlsr-300m-nepali/checkpoint-20400] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to wav2vec2-large-xlsr-300m-nepali/checkpoint-21600\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-21600/config.json\n",
      "Model weights saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-21600/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-21600/preprocessor_config.json\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/preprocessor_config.json\n",
      "Deleting older checkpoint [wav2vec2-large-xlsr-300m-nepali/checkpoint-20800] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to wav2vec2-large-xlsr-300m-nepali/checkpoint-22000\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-22000/config.json\n",
      "Model weights saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-22000/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-22000/preprocessor_config.json\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/preprocessor_config.json\n",
      "Deleting older checkpoint [wav2vec2-large-xlsr-300m-nepali/checkpoint-21200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to wav2vec2-large-xlsr-300m-nepali/checkpoint-22400\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-22400/config.json\n",
      "Model weights saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-22400/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-22400/preprocessor_config.json\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/preprocessor_config.json\n",
      "Deleting older checkpoint [wav2vec2-large-xlsr-300m-nepali/checkpoint-21600] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to wav2vec2-large-xlsr-300m-nepali/checkpoint-22800\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-22800/config.json\n",
      "Model weights saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-22800/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-22800/preprocessor_config.json\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/preprocessor_config.json\n",
      "Deleting older checkpoint [wav2vec2-large-xlsr-300m-nepali/checkpoint-22000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to wav2vec2-large-xlsr-300m-nepali/checkpoint-23200\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-23200/config.json\n",
      "Model weights saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-23200/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-23200/preprocessor_config.json\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/preprocessor_config.json\n",
      "Deleting older checkpoint [wav2vec2-large-xlsr-300m-nepali/checkpoint-22400] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to wav2vec2-large-xlsr-300m-nepali/checkpoint-23600\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-23600/config.json\n",
      "Model weights saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-23600/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-23600/preprocessor_config.json\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/preprocessor_config.json\n",
      "Deleting older checkpoint [wav2vec2-large-xlsr-300m-nepali/checkpoint-22800] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to wav2vec2-large-xlsr-300m-nepali/checkpoint-24000\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-24000/config.json\n",
      "Model weights saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-24000/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-24000/preprocessor_config.json\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/preprocessor_config.json\n",
      "Deleting older checkpoint [wav2vec2-large-xlsr-300m-nepali/checkpoint-23200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to wav2vec2-large-xlsr-300m-nepali/checkpoint-24400\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-24400/config.json\n",
      "Model weights saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-24400/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-24400/preprocessor_config.json\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/preprocessor_config.json\n",
      "Deleting older checkpoint [wav2vec2-large-xlsr-300m-nepali/checkpoint-23600] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to wav2vec2-large-xlsr-300m-nepali/checkpoint-24800\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-24800/config.json\n",
      "Model weights saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-24800/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-24800/preprocessor_config.json\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/preprocessor_config.json\n",
      "Deleting older checkpoint [wav2vec2-large-xlsr-300m-nepali/checkpoint-24000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to wav2vec2-large-xlsr-300m-nepali/checkpoint-25200\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-25200/config.json\n",
      "Model weights saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-25200/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-25200/preprocessor_config.json\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/preprocessor_config.json\n",
      "Deleting older checkpoint [wav2vec2-large-xlsr-300m-nepali/checkpoint-24400] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to wav2vec2-large-xlsr-300m-nepali/checkpoint-25600\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-25600/config.json\n",
      "Model weights saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-25600/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-25600/preprocessor_config.json\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/preprocessor_config.json\n",
      "Deleting older checkpoint [wav2vec2-large-xlsr-300m-nepali/checkpoint-24800] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to wav2vec2-large-xlsr-300m-nepali/checkpoint-26000\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-26000/config.json\n",
      "Model weights saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-26000/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-26000/preprocessor_config.json\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/preprocessor_config.json\n",
      "Deleting older checkpoint [wav2vec2-large-xlsr-300m-nepali/checkpoint-25200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to wav2vec2-large-xlsr-300m-nepali/checkpoint-26400\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-26400/config.json\n",
      "Model weights saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-26400/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-26400/preprocessor_config.json\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/preprocessor_config.json\n",
      "Deleting older checkpoint [wav2vec2-large-xlsr-300m-nepali/checkpoint-25600] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to wav2vec2-large-xlsr-300m-nepali/checkpoint-26800\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-26800/config.json\n",
      "Model weights saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-26800/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-26800/preprocessor_config.json\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/preprocessor_config.json\n",
      "Deleting older checkpoint [wav2vec2-large-xlsr-300m-nepali/checkpoint-26000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to wav2vec2-large-xlsr-300m-nepali/checkpoint-27200\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-27200/config.json\n",
      "Model weights saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-27200/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-27200/preprocessor_config.json\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/preprocessor_config.json\n",
      "Deleting older checkpoint [wav2vec2-large-xlsr-300m-nepali/checkpoint-26400] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to wav2vec2-large-xlsr-300m-nepali/checkpoint-27600\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-27600/config.json\n",
      "Model weights saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-27600/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-27600/preprocessor_config.json\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/preprocessor_config.json\n",
      "Deleting older checkpoint [wav2vec2-large-xlsr-300m-nepali/checkpoint-26800] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to wav2vec2-large-xlsr-300m-nepali/checkpoint-28000\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-28000/config.json\n",
      "Model weights saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-28000/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-28000/preprocessor_config.json\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/preprocessor_config.json\n",
      "Deleting older checkpoint [wav2vec2-large-xlsr-300m-nepali/checkpoint-27200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to wav2vec2-large-xlsr-300m-nepali/checkpoint-28400\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-28400/config.json\n",
      "Model weights saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-28400/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-28400/preprocessor_config.json\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/preprocessor_config.json\n",
      "Deleting older checkpoint [wav2vec2-large-xlsr-300m-nepali/checkpoint-27600] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to wav2vec2-large-xlsr-300m-nepali/checkpoint-28800\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-28800/config.json\n",
      "Model weights saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-28800/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-28800/preprocessor_config.json\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/preprocessor_config.json\n",
      "Deleting older checkpoint [wav2vec2-large-xlsr-300m-nepali/checkpoint-28000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to wav2vec2-large-xlsr-300m-nepali/checkpoint-29200\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-29200/config.json\n",
      "Model weights saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-29200/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-29200/preprocessor_config.json\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/preprocessor_config.json\n",
      "Deleting older checkpoint [wav2vec2-large-xlsr-300m-nepali/checkpoint-28400] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to wav2vec2-large-xlsr-300m-nepali/checkpoint-29600\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-29600/config.json\n",
      "Model weights saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-29600/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-29600/preprocessor_config.json\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/preprocessor_config.json\n",
      "Deleting older checkpoint [wav2vec2-large-xlsr-300m-nepali/checkpoint-28800] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to wav2vec2-large-xlsr-300m-nepali/checkpoint-30000\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-30000/config.json\n",
      "Model weights saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-30000/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-30000/preprocessor_config.json\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/preprocessor_config.json\n",
      "Deleting older checkpoint [wav2vec2-large-xlsr-300m-nepali/checkpoint-29200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to wav2vec2-large-xlsr-300m-nepali/checkpoint-30400\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-30400/config.json\n",
      "Model weights saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-30400/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-30400/preprocessor_config.json\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/preprocessor_config.json\n",
      "Deleting older checkpoint [wav2vec2-large-xlsr-300m-nepali/checkpoint-29600] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to wav2vec2-large-xlsr-300m-nepali/checkpoint-30800\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-30800/config.json\n",
      "Model weights saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-30800/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-30800/preprocessor_config.json\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/preprocessor_config.json\n",
      "Deleting older checkpoint [wav2vec2-large-xlsr-300m-nepali/checkpoint-30000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to wav2vec2-large-xlsr-300m-nepali/checkpoint-31200\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-31200/config.json\n",
      "Model weights saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-31200/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-31200/preprocessor_config.json\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/preprocessor_config.json\n",
      "Deleting older checkpoint [wav2vec2-large-xlsr-300m-nepali/checkpoint-30400] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to wav2vec2-large-xlsr-300m-nepali/checkpoint-31600\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-31600/config.json\n",
      "Model weights saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-31600/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-31600/preprocessor_config.json\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/preprocessor_config.json\n",
      "Deleting older checkpoint [wav2vec2-large-xlsr-300m-nepali/checkpoint-30800] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to wav2vec2-large-xlsr-300m-nepali/checkpoint-32000\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-32000/config.json\n",
      "Model weights saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-32000/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-32000/preprocessor_config.json\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/preprocessor_config.json\n",
      "Deleting older checkpoint [wav2vec2-large-xlsr-300m-nepali/checkpoint-31200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to wav2vec2-large-xlsr-300m-nepali/checkpoint-32400\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-32400/config.json\n",
      "Model weights saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-32400/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-32400/preprocessor_config.json\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/preprocessor_config.json\n",
      "Deleting older checkpoint [wav2vec2-large-xlsr-300m-nepali/checkpoint-31600] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to wav2vec2-large-xlsr-300m-nepali/checkpoint-32800\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-32800/config.json\n",
      "Model weights saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-32800/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-32800/preprocessor_config.json\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/preprocessor_config.json\n",
      "Deleting older checkpoint [wav2vec2-large-xlsr-300m-nepali/checkpoint-32000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to wav2vec2-large-xlsr-300m-nepali/checkpoint-33200\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-33200/config.json\n",
      "Model weights saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-33200/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-33200/preprocessor_config.json\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/preprocessor_config.json\n",
      "Deleting older checkpoint [wav2vec2-large-xlsr-300m-nepali/checkpoint-32400] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to wav2vec2-large-xlsr-300m-nepali/checkpoint-33600\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-33600/config.json\n",
      "Model weights saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-33600/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-33600/preprocessor_config.json\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/preprocessor_config.json\n",
      "Deleting older checkpoint [wav2vec2-large-xlsr-300m-nepali/checkpoint-32800] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to wav2vec2-large-xlsr-300m-nepali/checkpoint-34000\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-34000/config.json\n",
      "Model weights saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-34000/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-34000/preprocessor_config.json\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/preprocessor_config.json\n",
      "Deleting older checkpoint [wav2vec2-large-xlsr-300m-nepali/checkpoint-33200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to wav2vec2-large-xlsr-300m-nepali/checkpoint-34400\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-34400/config.json\n",
      "Model weights saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-34400/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-34400/preprocessor_config.json\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/preprocessor_config.json\n",
      "Deleting older checkpoint [wav2vec2-large-xlsr-300m-nepali/checkpoint-33600] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to wav2vec2-large-xlsr-300m-nepali/checkpoint-34800\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-34800/config.json\n",
      "Model weights saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-34800/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-34800/preprocessor_config.json\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/preprocessor_config.json\n",
      "Deleting older checkpoint [wav2vec2-large-xlsr-300m-nepali/checkpoint-34000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to wav2vec2-large-xlsr-300m-nepali/checkpoint-35200\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-35200/config.json\n",
      "Model weights saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-35200/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/checkpoint-35200/preprocessor_config.json\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/preprocessor_config.json\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-3435b262f1ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1438\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_step_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1440\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_log_save_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1441\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1442\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_substep_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1568\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_save\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1569\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1570\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_save_checkpoint\u001b[0;34m(self, model, trial, metrics)\u001b[0m\n\u001b[1;32m   1718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1719\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush_to_hub\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1720\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_push_from_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1722\u001b[0m         \u001b[0;31m# Maybe delete some older checkpoints.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_push_from_checkpoint\u001b[0;34m(self, checkpoint_folder)\u001b[0m\n\u001b[1;32m   2739\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2740\u001b[0m                 \u001b[0mcommit_message\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Training in progress, epoch {int(self.state.epoch)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2741\u001b[0;31m             _, self.push_in_progress = self.repo.push_to_hub(\n\u001b[0m\u001b[1;32m   2742\u001b[0m                 \u001b[0mcommit_message\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcommit_message\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauto_lfs_prune\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2743\u001b[0m             )\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/huggingface_hub/repository.py\u001b[0m in \u001b[0;36mpush_to_hub\u001b[0;34m(self, commit_message, blocking, clean_ok, auto_lfs_prune)\u001b[0m\n\u001b[1;32m   1249\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgit_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauto_lfs_track\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1250\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgit_commit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommit_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1251\u001b[0;31m         return self.git_push(\n\u001b[0m\u001b[1;32m   1252\u001b[0m             \u001b[0mupstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"origin {self.current_branch}\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0mblocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mblocking\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/huggingface_hub/repository.py\u001b[0m in \u001b[0;36mgit_push\u001b[0;34m(self, upstream, blocking, auto_lfs_prune)\u001b[0m\n\u001b[1;32m   1016\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mreturn_code\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1018\u001b[0;31m                         raise subprocess.CalledProcessError(\n\u001b[0m\u001b[1;32m   1019\u001b[0m                             \u001b[0mreturn_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m                         )\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/huggingface_hub/repository.py\u001b[0m in \u001b[0;36mlfs_log_progress\u001b[0;34m()\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m             \u001b[0mexit_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m             \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"GIT_LFS_PROGRESS\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_lfs_progress_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1011\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1012\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1025\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# already determined that the C code is done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1027\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1028\u001b[0m             \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1029\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ad8666a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to wav2vec2-large-xlsr-300m-nepali\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/config.json\n",
      "Model weights saved in wav2vec2-large-xlsr-300m-nepali/pytorch_model.bin\n",
      "Configuration saved in wav2vec2-large-xlsr-300m-nepali/preprocessor_config.json\n",
      "Dropping the following result as it does not have all the necessary fields:\n",
      "{}\n",
      "Several commits (2) will be pushed upstream.\n",
      "The progress bars may be unreliable.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c1ac06138ee48cf99eac090fb6dc1cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file pytorch_model.bin:   0%|          | 3.37k/1.18G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://huggingface.co/prajin/wav2vec2-large-xlsr-300m-nepali\n",
      "   2889e67..e7edbaf  main -> main\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b41ace29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.save_metrics(all, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f8de76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
