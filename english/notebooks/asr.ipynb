{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cc31e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import torch \n",
    "import torch.nn as nn \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5170ea18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "import torch.optim as optim \n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfeae334",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc612cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ea8f3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_wer(wer_scores, combined_ref_len):\n",
    "    return float(sum(wer_scores)) / float(combined_ref_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "325c6290",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _levenshtein_distance(ref, hyp):\n",
    "    \"\"\"\n",
    "    Levenshtein distance is used for measruging the difference between two seq. \n",
    "    It is the minimum number of single character editds ( subs, insert, delete,) req to change one word into other.\n",
    "    \n",
    "    \"\"\"\n",
    "    m = len(ref)\n",
    "    n = len(hyp)\n",
    "    \n",
    "    if ref == hyp :\n",
    "        return   0\n",
    "    if m == 0 :\n",
    "         return n \n",
    "    if n == 0:\n",
    "        return m \n",
    "    \n",
    "    if (m < n):\n",
    "        ref, hyp = hyp, ref\n",
    "        m, n = n, m\n",
    "        \n",
    "    distance = np.zeros((2, n+1), dtype=np.int32)\n",
    "    \n",
    "    for j in range(0, n+1):\n",
    "        distance[0][j] =1 \n",
    "        \n",
    "    for i in range(1, m+1):\n",
    "        prev_row_idx = (i - 1) % 2\n",
    "        cur_row_idx = (i - 1) % 2\n",
    "        distance[cur_row_idx][0] = 1 \n",
    "        for j in range(1, n+1):\n",
    "            if(ref[i-1] == hyp[j-1]):\n",
    "                distance[cur_row_idx][j] = distance[prev_row_idx][j - 1]\n",
    "            else:\n",
    "                s_num = distance[prev_row_idx][j - 1] + 1\n",
    "                i_num = distance[cur_row_idx][j - 1] + 1\n",
    "                d_num = distance[prev_row_idx][j] + 1\n",
    "                distance[cur_row_idx][j] = min(s_num, i_num, d_num)\n",
    "\n",
    "    return distance[m % 2][n]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2911b1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_errors(reference, hypothesis, ignore_case = False, delimiter = ' '):\n",
    "    \"\"\"\n",
    "    Computer Levenshtein distance between reference seq and hyp. seq un word level.\n",
    "    parameters:\n",
    "        :reference: The reference sentence \n",
    "        :hypothesis: The hypothesis sentence \n",
    "        :ignore_case: Whether case sensitive or not\n",
    "        : delimeter: Delimer of input setence \n",
    "        :return: Lev Distance and word number of ref Sentence:\n",
    "    \"\"\"\n",
    "    if ignore_case == True:\n",
    "        reference = reference.lower()\n",
    "        hypothesis = hypothesis.lower()\n",
    "        \n",
    "        \n",
    "    ref_words = reference.split(delimiter)\n",
    "    hyp_words = hypothesis.split(delimiter)\n",
    "    \n",
    "    \n",
    "    edit_distance = _levenshtein_distance(ref_words, hyp_words)\n",
    "    return float(edit_distance), len(ref_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d1673cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_errors(reference, hypothesis, ignore_case=False, ignore_space=False):\n",
    "    \"\"\"\n",
    "    Computing ref distance between reference and hyptheses seq in char level.\n",
    "    \n",
    "    Paramters:\n",
    "    reference: reference sentence \n",
    "    hypothesos: hypothesis sentence \n",
    "    ignore_case: Case sensitive or not \n",
    "    remove_space: remove internal space characters\n",
    "    return lev distance and length of reg setnece\n",
    "    \"\"\"\n",
    "    \n",
    "    if ignore_case == True:\n",
    "        reference = reference.lower()\n",
    "        hypothesis = hypothesis.lower()\n",
    "        \n",
    "        \n",
    "    join_char = ' '\n",
    "    if remove_space == True:\n",
    "        join_char = ''\n",
    "    reference = join_char.join(filter(None, reference.split(' ')))\n",
    "    hypothesis = join_char.join(filter(None, hypothesis.split(' ')))\n",
    "    \n",
    "    edit_distance = _levenshtein_distance(reference, hypothesis)\n",
    "    return float(edit_distance), len(reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f1129e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wer(reference, hypothesis, ignore_case = False, delimiter = ' '):\n",
    "    \n",
    "    \"\"\"\n",
    "    Calculating the word error rate. \n",
    "    It compares rthe referenxce text and hyp text. \n",
    "    \n",
    "    WER is defined as :\n",
    "    WER = ( Sw + Dw + Iw) / Nw\n",
    "    \n",
    "    Where:\n",
    "        Sw: Number of Substitued words.\n",
    "        Dw: Number of Deleted words \n",
    "        Iw: Number of Inserted words\n",
    "        Nw: Number of words in reference.\n",
    "    \"\"\"\n",
    "    \n",
    "    edit_distance, ref_len = word_errors(reference, hypothesis, ignore_case, delimiter)\n",
    "    \n",
    "    if ref_len == 0:\n",
    "        raise ValueError(\"Refernec word number must be greater than 0\")\n",
    "    wer = float(edit_distance)/ ref_len\n",
    "    return wer \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57cb993d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cer(reference, hypothesis, ignore_case=False, remove_space=False):\n",
    "    \"\"\"\n",
    "    Calculating the Character error rarte. \n",
    "    It is defined as :\n",
    "    CER = (Sc + Dc + Ic) / Nc\n",
    "    Where \n",
    "    Sc: Number of characters substitued\n",
    "    Dc: number of characters deleted\n",
    "    Ic: Number of characters inserted.\n",
    "    Nc: Numvber of characrer in ref.\n",
    "    \"\"\"\n",
    "    \n",
    "    if ref_len == 0:\n",
    "        raise ValueError(\"Refernec word number must be greater than 0\")\n",
    "        \n",
    "    cer = float(edit_distance) / ref_len\n",
    "    return cer\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "afdbe9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextTransform:\n",
    "    \"\"\"\n",
    "    Maps characters to integers and vice versa\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        char_map_str = \"\"\"\n",
    "        ' 0\n",
    "        <SPACE> 1\n",
    "        a 2\n",
    "        b 3\n",
    "        c 4\n",
    "        d 5\n",
    "        e 6\n",
    "        f 7\n",
    "        g 8\n",
    "        h 9\n",
    "        i 10\n",
    "        j 11\n",
    "        k 12\n",
    "        l 13\n",
    "        m 14\n",
    "        n 15\n",
    "        o 16\n",
    "        p 17\n",
    "        q 18\n",
    "        r 19\n",
    "        s 20\n",
    "        t 21\n",
    "        u 22\n",
    "        v 23\n",
    "        w 24\n",
    "        x 25\n",
    "        y 26\n",
    "        z 27\n",
    "        \"\"\"\n",
    "        \n",
    "        self.char_map = {}\n",
    "        self.index_map = {}\n",
    "        for line in char_map_str.strip().split(\"\\n\"):\n",
    "            ch, index = line.split()\n",
    "            self.char_map[ch]  = int(index)\n",
    "            self.index_map[int(index)] = ch \n",
    "            \n",
    "        self.index_map[1]  =  ' '\n",
    "        \n",
    "        \n",
    "        \n",
    "    def text_to_int(self, text):\n",
    "        \"\"\"\n",
    "        Using character mapping , convert the text to integer seq\n",
    "        \n",
    "        \"\"\"\n",
    "        int_sequence = []\n",
    "        for c in text:\n",
    "            if c == ' ':\n",
    "                ch = self.char_map['<SPACE>']\n",
    "                \n",
    "            else:\n",
    "                ch = self.char_map[c]\n",
    "            int_sequence.append(ch)\n",
    "            \n",
    "        return int_sequence\n",
    "    \n",
    "    \n",
    "    def int_to_text(self, labels):\n",
    "        \"\"\"\n",
    "        Using char map convert intefer label to text\n",
    "        \"\"\"\n",
    "        string = []\n",
    "        for i in labels:\n",
    "            string.append(self.char_map[[i]])\n",
    "            \n",
    "        return ''.join(string).replace('<SPACE>', ' ')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce418cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/prajin/miniforge3/lib/python3.9/site-packages/torchaudio/functional/functional.py:594: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_audio_transform = nn.Sequential(\n",
    "                            torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=128),\n",
    "                            torchaudio.transforms.FrequencyMasking(freq_mask_param=30),\n",
    "                            torchaudio.transforms.TimeMasking(time_mask_param=100)\n",
    ")\n",
    "\n",
    "valid_audio_transform = torchaudio.transforms.MelSpectrogram()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "11500c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_transforms  = TextTransform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0f947162",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_processing(data, data_type='train'):\n",
    "    spectrograms = []\n",
    "    labels = []\n",
    "    input_lengths = []\n",
    "    label_lengths =[]\n",
    "    for(waveform, _, utterance, _, _, _) in data:\n",
    "        if(data_type=='train'):\n",
    "            spec = train_audio_transform(waveform).squeeze(0).transpose(0, 1)\n",
    "        elif (data_type == 'valid'):\n",
    "            spec = valid_audio_transform(waveform).squeeze(0).transpose(0,1)\n",
    "        else:\n",
    "            raise Exception('data_type should be train or valid')\n",
    "            \n",
    "        spectrograms.append(spec)\n",
    "        label = torch.Tensor(text_transforms.text_to_int(utterance.lower()))\n",
    "        labels.append(label)\n",
    "        input_lengths.append(spec.shape[0]//2)\n",
    "        label_lengths.append(len(label))\n",
    "        \n",
    "    spectrograms = nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2,3)\n",
    "    labels = nn.utils.rnn.pad_sequence(labels, batch_first=True)\n",
    "    \n",
    "    return spectrograms, labels, input_lengths, label_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "10ed162b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GreedyDecoder(output, labels, label_lengths, blank_label = 28,collapse_repeated=True):\n",
    "    arg_maxexs = torch.argmax(output, dim=2)\n",
    "    decodes = []\n",
    "    targets  = []\n",
    "    for i, args in enumerate(arg_maxexs):\n",
    "        decode = []\n",
    "        targets.append(text_transforms.text_to_int(labels[i][:label_lengths[i]].tolist()))\n",
    "        for j, index in enumerate(args):\n",
    "            if(index!= blank_label):\n",
    "                if (collapse_repeated and j != 0 and index == args[j-1]):\n",
    "                    continue \n",
    "                decode.append(index.item())\n",
    "        decodes.append(text_transforms.int_to_text(decode))\n",
    "        \n",
    "    return decodes, target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52eab1e5",
   "metadata": {},
   "source": [
    "# Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "93a23053",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNLayerNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    Layer norm for cnn input\n",
    "    \"\"\"\n",
    "    def __init__(self, n_feats):\n",
    "        super(CNNLayerNorm, self).__init__()\n",
    "        self.layer_norm = nn.LayerNorm(n_feats)\n",
    "        \n",
    "    def forward(self, x):\n",
    "#         x (batch, channel, feature, time)\n",
    "        x = x.tranpose(2, 3).contiguous() #      batch ,channel, time, feature \n",
    "        x = self.layer_norm(x)\n",
    "        return x.tranpose(2,3).contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a263ce63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualCNN(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel, stride, dropout, n_feats):\n",
    "        super(ResidualCNN, self).__init__()\n",
    "        \n",
    "        self.cnn1 = nn.Conv2d(in_channels, out_channels, kernel, stride, padding=kernel//2)\n",
    "        self.cnn2 = nn.Conv2d(out_channels, out_channels, kernel, stride, padding= kernel//2)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.layer_nomr1 = CNNLayerNorm(n_feats)\n",
    "        self.layer_nomr2 = CNNLayerNorm(n_feats)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x \n",
    "        x = self.layer_nomr1(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.dropout1(1)\n",
    "        x  = self.cnn1(x)\n",
    "        \n",
    "        x = self.layer_norm2(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.cnn2(x)\n",
    "        x += residual\n",
    "        return x # (batch, channel, feature, time)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7293b230",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BidirectionalGRU(nn.Module):\n",
    "    def __init__(self, rnn_dim, hidden_size, dropout, batch_first):\n",
    "        super(BidirectionalGRU, self).__init__()\n",
    "        \n",
    "        self.BiGRU = nn.GRU(\n",
    "            input_size = rnn_dim, \n",
    "            hidden_size = hidden_size,\n",
    "            num_layers  = 1,\n",
    "            batch_first = batch_first,\n",
    "            bidirectional = True\n",
    "        )\n",
    "        self.layer_norm = nn.LayerNorm(rnn_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x  = self.layer_norm(x)\n",
    "        x = F.gelu(x)\n",
    "        x, _ = self.BiGRU(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "66e27857",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNLayerNorm(nn.Module):\n",
    "    \"\"\"Layer normalization built for cnns input\"\"\"\n",
    "    def __init__(self, n_feats):\n",
    "        super(CNNLayerNorm, self).__init__()\n",
    "        self.layer_norm = nn.LayerNorm(n_feats)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x (batch, channel, feature, time)\n",
    "        x = x.transpose(2, 3).contiguous() # (batch, channel, time, feature)\n",
    "        x = self.layer_norm(x)\n",
    "        return x.transpose(2, 3).contiguous() # (batch, channel, feature, time) \n",
    "\n",
    "\n",
    "class ResidualCNN(nn.Module):\n",
    "    \"\"\"Residual CNN inspired by https://arxiv.org/pdf/1603.05027.pdf\n",
    "        except with layer norm instead of batch norm\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel, stride, dropout, n_feats):\n",
    "        super(ResidualCNN, self).__init__()\n",
    "\n",
    "        self.cnn1 = nn.Conv2d(in_channels, out_channels, kernel, stride, padding=kernel//2)\n",
    "        self.cnn2 = nn.Conv2d(out_channels, out_channels, kernel, stride, padding=kernel//2)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.layer_norm1 = CNNLayerNorm(n_feats)\n",
    "        self.layer_norm2 = CNNLayerNorm(n_feats)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x  # (batch, channel, feature, time)\n",
    "        x = self.layer_norm1(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.cnn1(x)\n",
    "        x = self.layer_norm2(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.cnn2(x)\n",
    "        x += residual\n",
    "        return x # (batch, channel, feature, time)\n",
    "\n",
    "\n",
    "class BidirectionalGRU(nn.Module):\n",
    "\n",
    "    def __init__(self, rnn_dim, hidden_size, dropout, batch_first):\n",
    "        super(BidirectionalGRU, self).__init__()\n",
    "\n",
    "        self.BiGRU = nn.GRU(\n",
    "            input_size=rnn_dim, hidden_size=hidden_size,\n",
    "            num_layers=1, batch_first=batch_first, bidirectional=True)\n",
    "        self.layer_norm = nn.LayerNorm(rnn_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer_norm(x)\n",
    "        x = F.gelu(x)\n",
    "        x, _ = self.BiGRU(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SpeechRecognitionModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_cnn_layers, n_rnn_layers, rnn_dim, n_class, n_feats, stride=2, dropout=0.1):\n",
    "        super(SpeechRecognitionModel, self).__init__()\n",
    "        n_feats = n_feats//2\n",
    "        self.cnn = nn.Conv2d(1, 32, 3, stride=stride, padding=3//2)  # cnn for extracting heirachal features\n",
    "\n",
    "        # n residual cnn layers with filter size of 32\n",
    "        self.rescnn_layers = nn.Sequential(*[\n",
    "            ResidualCNN(32, 32, kernel=3, stride=1, dropout=dropout, n_feats=n_feats) \n",
    "            for _ in range(n_cnn_layers)\n",
    "        ])\n",
    "        self.fully_connected = nn.Linear(n_feats*32, rnn_dim)\n",
    "        self.birnn_layers = nn.Sequential(*[\n",
    "            BidirectionalGRU(rnn_dim=rnn_dim if i==0 else rnn_dim*2,\n",
    "                             hidden_size=rnn_dim, dropout=dropout, batch_first=i==0)\n",
    "            for i in range(n_rnn_layers)\n",
    "        ])\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(rnn_dim*2, rnn_dim),  # birnn returns rnn_dim*2\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(rnn_dim, n_class)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = self.rescnn_layers(x)\n",
    "        sizes = x.size()\n",
    "        x = x.view(sizes[0], sizes[1] * sizes[2], sizes[3])  # (batch, feature, time)\n",
    "        x = x.transpose(1, 2) # (batch, time, feature)\n",
    "        x = self.fully_connected(x)\n",
    "        x = self.birnn_layers(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e02fa55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IterMeter(object):\n",
    "    \"\"\"\n",
    "    Keeping track of total iterations\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.val =  0 \n",
    "        \n",
    "    def step(self):\n",
    "        self.val += 1 \n",
    "        \n",
    "    def get(self):\n",
    "        return self.val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c3fedaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter):\n",
    "    model.train()\n",
    "    data_len = len(train_loader.dataset)\n",
    "    for batch_idx, _data in enumerate(train_loader):\n",
    "        spectrograms, labels, input_lengths, label_lengths = _data \n",
    "        spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(spectrograms)\n",
    "        output = F.log_softmax(output, dim=2)\n",
    "        output = output.transpose(0, 1)\n",
    "        \n",
    "        loss = criterion(output, labels, input_lengths, label_lengths)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        iter_meter.step()\n",
    "        \n",
    "        if batch_idx%100 ==0 or batch_idx == data_len:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(spectrograms), data_len,\n",
    "                    100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2c768c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader, criterion, epoch, iter_meter):\n",
    "    print(\"\\n Evaluation \\n\")\n",
    "    model.eval()\n",
    "    test_loss = 0 \n",
    "    test_cer, test_wer = [], []\n",
    "    with torch.no_grad():\n",
    "        for i, _data in enumerate(test_loader):\n",
    "            spectrograms, labels, input_lengths, label_lengths = _data\n",
    "            spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "            \n",
    "            output = model(spectrograms)\n",
    "            output = F.log_softmax(output, dim=2)\n",
    "            output = output.transpose(0, 1)\n",
    "            \n",
    "            loss = criterion(output, labels, input_lengths, label_lengths)\n",
    "            test_loss += loss.item()  / len(test_loader)\n",
    "            \n",
    "            decoded_preds, decoded_targets = GreedyDecoder(output.tranpose(0,1), labels, label_lengths)\n",
    "            \n",
    "            for j in range(len(decoded_preds)):\n",
    "                test_cer.append(cer(decoded_targets[j], decoded_preds[j]))\n",
    "                test_wer.append(wer(decoded_targets[j], decoded_preds[j]))\n",
    "                \n",
    "    avg_cer = sum(test_cer)/len(test_cer)\n",
    "    avg_wer = sum(test_wer)/len(test_wer)\n",
    "    print('Test set: Average loss: {:.4f}, Average CER: {:4f} Average WER: {:.4f}\\n'.format(test_loss, avg_cer, avg_wer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "56cd6f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(learning_rate = 5e-4, batch_size=20, epochs=10,train_url= \"dev-clean\",\n",
    "        test_url = \"test-clean\"):\n",
    "    hparams = {\n",
    "        \"n_cnn_layers\": 3,\n",
    "        \"n_rnn_layers\": 5,\n",
    "        \"rnn_dim\": 512,\n",
    "        \"n_class\": 29,\n",
    "        \"n_feats\": 128,\n",
    "        \"stride\":2,\n",
    "        \"dropout\": 0.1,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"epochs\": epochs\n",
    "    }\n",
    "    \n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    torch.manual_seed(7)\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    if not os.path.isdir(\"./data\"):\n",
    "        os.makedirs(\"./data\")\n",
    "        \n",
    "        \n",
    "    train_dataset = torchaudio.datasets.LIBRISPEECH(\"./data\", url=train_url, download=True)\n",
    "    test_dataset = torchaudio.datasets.LIBRISPEECH(\"./data\", url=test_url, download=True)\n",
    "\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "    train_loader = data.DataLoader(dataset=train_dataset,\n",
    "                                batch_size=hparams['batch_size'],\n",
    "                                shuffle=True,\n",
    "                                collate_fn=lambda x: data_processing(x, 'train'),\n",
    "                                **kwargs)\n",
    "    test_loader = data.DataLoader(dataset=test_dataset,\n",
    "                                batch_size=hparams['batch_size'],\n",
    "                                shuffle=False,\n",
    "                                collate_fn=lambda x: data_processing(x, 'valid'),\n",
    "                                **kwargs)\n",
    "\n",
    "        \n",
    "        \n",
    "    model = SpeechRecognitionModel(\n",
    "        hparams['n_cnn_layers'], hparams['n_rnn_layers'], hparams['rnn_dim'],\n",
    "        hparams['n_class'], hparams['n_feats'], hparams['stride'], hparams['dropout']\n",
    "        ).to(device)\n",
    "    \n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters(), hparams['learning_rate'])\n",
    "    criterion = nn.CTCLoss(blank=28).to(device)\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=hparams['learning_rate'], \n",
    "                                            steps_per_epoch=int(len(train_loader)),\n",
    "                                            epochs=hparams['epochs'],\n",
    "                                            anneal_strategy='linear')\n",
    "    \n",
    "    iter_meter = IterMeter()\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train(model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter)\n",
    "        test(model, device, test_loader, criterion, epoch, iter_meter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6451e581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/2703 (0%)]\tLoss: 6.263378\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/3f/t2z7k3f5209_6hp246wpbpg40000gn/T/ipykernel_19882/4207754299.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlibri_test_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"test-clean\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibri_train_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibri_test_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/3f/t2z7k3f5209_6hp246wpbpg40000gn/T/ipykernel_19882/374519446.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(learning_rate, batch_size, epochs, train_url, test_url)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0miter_meter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIterMeter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter_meter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter_meter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/3f/t2z7k3f5209_6hp246wpbpg40000gn/T/ipykernel_19882/838979199.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learning_rate = 5e-4\n",
    "batch_size = 10\n",
    "epochs = 10\n",
    "libri_train_set = \"dev-clean\"\n",
    "libri_test_set = \"test-clean\"\n",
    "\n",
    "main(learning_rate, batch_size, epochs, libri_train_set, libri_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b19b22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9b0666",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d429fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f728886",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f3463f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c68cb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ebf2d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f0f4b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f8cd9d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_url=\"dev-clean\"\n",
    "train_dataset = torchaudio.datasets.LIBRISPEECH(\"./data\", url=train_url, download=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c1f419c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torchaudio.datasets.librispeech.LIBRISPEECH at 0x15d40dca0>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a3f1edbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hparams = {\n",
    "        \"n_cnn_layers\": 3,\n",
    "        \"n_rnn_layers\": 5,\n",
    "        \"rnn_dim\": 512,\n",
    "        \"n_class\": 29,\n",
    "        \"n_feats\": 128,\n",
    "        \"stride\":2,\n",
    "        \"dropout\": 0.1,\n",
    "        \"learning_rate\": 0.01,\n",
    "        \"batch_size\": 20,\n",
    "        \"epochs\": 10\n",
    "    }\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "train_loader = data.DataLoader(dataset=train_dataset,\n",
    "                                batch_size=hparams['batch_size'],\n",
    "                                shuffle=True,\n",
    "                                collate_fn=lambda x: data_processing(x, 'train'),\n",
    "                                **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "18205888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 309])\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, _data in enumerate(train_loader):\n",
    "    spectrograms, labels, input_lengths, label_lengths = _data \n",
    "    print(labels.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e5a093",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
